\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{subfigure}

\title{MTH 9855 Homework 6}
\author{Junliang Zhou}

\begin{document}
\maketitle

\section{Problem 6.1}

Use the Sherman-Morrison-Woodbury matrix inversion lemma to derive a simple expression for the inverse of the covariance matrix in an APT model. In other words, derive an expression for $\Sigma^{-1}$ where

\[\Sigma=\mathbb{V}[R]=XFX'+D\]
where $D$ is diagonal and $X$ is $n\times p$ and as usual we assume $p\ll n$. Any matrices being inverted should be either diagonal or $p\times p$.\newline

\textit{Proof.}\newline

By Sherman-Morrison-Woodbury matrix inversion lemma, we have
\[\Sigma^{-1}={(XFX'+D)}^{-1}=D^{-1}-D^{-1} X {(F^{-1}+X' D^{-1} X)}^{-1} X' D^{-1}\]

\section{Problem 6.2}

Show that, for any $n\times p$ real matrix $X$ (not necessarily full rank) and $n$-vector $Y$, the following are equal:

\begin{enumerate}[(a)]
\item $\lim_{\delta \to 0^{+}} {(X'X+\delta I)}^{-1} X'Y$
\item The minimum-norm element of $\text{arg}\min_b \lVert Y-Xb \rVert$
\item $VS^{+}U'Y$ where $X=USV'$ is the SVD of $X$
\end{enumerate}

\textit{Proof.}\newline

"(a)$\Leftrightarrow$(b)":\newline

By taking the first order condition, $(X'X+\delta I)^{-1}X'Y$ is chosen to minimize the following optimization problem,
\[\min_b \left(\lVert Y-Xb \rVert^2 + \delta \lVert b \rVert^2 \right)\]

That is,
\[(X'X+\delta I)^{-1}X'Y=\text{arg}\min_b \left(\lVert Y-Xb \rVert^2 + \delta \lVert b \rVert^2 \right)\]

where the optimization problem is equivalent to the Lagrange multiplier of the following constrained optimization problem,
\[\min_b \lVert Y-Xb \rVert^2\]
\[\text{s.t.}\quad \lVert b \rVert^2 \leq c,\quad c\in\mathbb{R}^{+}\]

As $\delta\to 0^{+}$, it is equivalent to that the constraint $c\to\infty$, the problem becomes unconstrained. Therefore, 
\[\lim_{\delta\to 0^{+}}(X'X+\delta I)^{-1} X'Y =\text{arg}\min_b \lVert Y-Xb \rVert\]

"(b)$\Leftrightarrow$(c)":\newline

From above we know that
\[X'Y=(X'X+\delta I)b\]

Substitute the SVD $X=USV'$ into the equation, and as $V$ is a orthogonal matrix,
\[VS'U'Y=(VS'SV'+\delta VIV')b=V(S'S+\delta I)V'b\]

Since $(S'S+\delta I)$ is guaranteed to be invertible even if $X$ is not full rank, we have
\[V (S'S+\delta I)^{-1} S'U'Y=b\]

As $\delta\to 0^{+}$, $b$ converges to $VS^{+}U'Y$ where
\[\lim_{\delta\to 0^{+}}(S'S+\delta I)^{-1} S'=S^{+}\]

which is one of the properties of pseudo-inverse matrices.\newline 

\end{document}